{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777bbbcf-eef2-4177-9d25-5f8ba9a2dd46",
   "metadata": {},
   "source": [
    "## Q3. Translating a cryptic language (30 marks)\n",
    "In a fictitious world, a group of scientists obtained a document containing a sample of texts written in a cryptic language, together with their English translations. However, they are unable to find a way of translating arbitrary texts from the cryptic language. Knowing that you have learned deep learning models for machine translation, they are approaching you for help.\n",
    "\n",
    "They have sent you a data file containing a training set and a test set.\n",
    "The code below illustrates how to use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a973d9-e90f-42e9-8443-87404589452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29721 training pairs and 9431 test pairs\n",
      "\n",
      "The third pair of training example:\n",
      "- Cryptic: T:aTmUB:BTzp:;)p,:Gp,M 1iU773g rvz:Um:7;,7o,pM r:B\n",
      "- English: a panic came over her. \"Kitty! I'm in torture. I c\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "data = pkl.load(open('cryptic.pkl', 'rb'))\n",
    "x_tr = data['train']['x']\n",
    "y_tr = data['train']['y']\n",
    "x_ts = data['test']['x']\n",
    "y_ts = data['test']['y']\n",
    "print('%d training pairs and %d test pairs\\n' % (len(x_tr), len(x_ts)))\n",
    "\n",
    "print('The third pair of training example:')\n",
    "print('- Cryptic:', x_tr[2])\n",
    "print('- English:', y_tr[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9034e590-034c-4648-ad46-e50c4f1899aa",
   "metadata": {},
   "source": [
    "As you can see, these texts are not necessarily complete sentences, and in fact, some words may be incomplete too.\n",
    "\n",
    "In this question, you will train a character-level machine translator using the encoder-decoder architecture as described in Lecture 18 - that is, the encoder takes in one letter at a time from the text in the cryptic language, and the decoder generates one letter at a time for its English translation.\n",
    "For this dataset, each short text, whether in the cryptic language or in English, has exactly 50 letters, which is convenient for batching processing using recurrent models.\n",
    "\n",
    "Specifically, as described in lecture, the encoder-decoder architecture has an encoder RNN $f^{e}$, a decoder RNN $f^{d}$, and a predictor $g$: \n",
    "\\begin{align*}\n",
    "    h^{e}_{t} &= f^{e}(h_{t-1}^{e}, x_{t}), \\\\\n",
    "    h^{d}_{t} &= f^{d}(h_{t}^{d}, y_{t-1}, c), \\\\\n",
    "    y_{t} &\\sim g(\\cdot | h_{t}^{d}, y_{t-1}, c).\n",
    "\\end{align*}\n",
    "For this problem, each input $x_{t}$ is the one-hot vector for a single letter in the cryptic text,\n",
    "each output $y_{t}$ is the one-hot vector for a single letter in the English text.\n",
    "As in general, the context vector $c$ is the last hidden state from the encoder, \n",
    "and the input to the decoder at the $t$-th time step is $(y_{t-1}, c)$.\n",
    "The decoder RNN behaves different during training and testing:\n",
    "* During training, each $y_{t-1}$ in the input $(y_{t-1}, c)$ to the decoder is the one-hot vector representing the $(t-1)$-th letter in the given English translation (assume $t=1$ is the first letter), except that $y_{0}$ is a one-hot vector representing a special SOS (start of sentence) letter.\n",
    "* During testing, each $y_{t-1}$ in the input $(y_{t-1}, c)$ to the decoder is obtained in a greedy way as the one-hot vector for the most likely letter predicted by the predictor $g$, except that $y_{0}$ represents SOS.\n",
    "\n",
    "**(a)** (5 marks)\n",
    "Complet the code for the `Lang` class below.\n",
    "Append an EOS letter '\\n' to each example cryptic/English text.\n",
    "Create `Lang` objects for both the cryptic language and English, and use them to convert texts to one-hot representations for both the training and test data.\n",
    "Make sure you add an additional SOS letter defined by `chr(0)` as an additional letter for the English language, as this is needed for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-utilization",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e54e7d5-835d-4736-8b3b-e3b5cffc5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, text):\n",
    "        self.int2char = dict(enumerate(sorted(set(text))))\n",
    "        self.char2int = {v: k for k, v in self.int2char.items()}\n",
    "        self.num_char = len(self.char2int)\n",
    "\n",
    "    def onehot(self, texts):\n",
    "        '''\n",
    "        Convert a list of strings to a list of sequences of one-hot vectors for letters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        texts: a list or array of str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a list, where output[i] is a list of one-hot vectors for the letters in texts[i]\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        pass\n",
    "\n",
    "    def text(self, onehots):\n",
    "        '''\n",
    "        Convert a list of sequences of one-hot vectors to a list of strings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        onehots: a list, where onehots[i] is a list of one-hot vectors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a list, where output[i] is a str corresponding to sequence of one-hot vectors in onehots[i]\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        pass\n",
    "\n",
    "# Task: create the Lang objects for the cryptic language and English, and convert the data to one-hot representations\n",
    "SOS = chr(0)\n",
    "EOS = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3931c80-765f-49f6-8e47-8957cccc242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELP: create your Lang objects and run some tests on them\n",
    "# cr = Lang object for the cryptic language\n",
    "# en = Lang object for English \n",
    "\n",
    "# run the code below to check whether the fields are correctly computed\n",
    "#en.num_char, en.char2int, en.int2char\n",
    "\n",
    "# run the code below to check the implementon of text and onehot - the output should be two True's\n",
    "# cr.text(cr.onehot(x_tr[:10])) == x_tr[:10], en.text(en.onehot(y_tr[:10])) == y_tr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-things",
   "metadata": {},
   "source": [
    "**(b)** (10 marks)\n",
    "Complete the code for the encoder-decoder model below.\n",
    "Write a short paragraph to briefly describe the type of architecture used\n",
    "(e.g. vanilla RNN, LSTM, GRU, MLP) for the encoder/decoder/predictor, and the hyperparameters chosen \n",
    "(e.g. number of layers, number of neurons for each layer, activation functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-binary",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "enhanced-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import LSTM\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    '''\n",
    "    Your code should support usages as illustrated in the code below, and you should run such \n",
    "    code to see if your model has any bug before proceeding to train it.\n",
    "    \n",
    "    net = Translator(en, cr)\n",
    "    # input the one-hot representation of some cryptic texts and their translations, and output\n",
    "    # the class probabilities at each time step.\n",
    "    p = net(x, y)\n",
    "    # input the one-hot representation of some cryptic texts, and output one-hot representation\n",
    "    # of their translations\n",
    "    yhat = net(x)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, en: Lang, cr: Lang):\n",
    "        '''\n",
    "        You can add additional arguments to this function if you want.\n",
    "        '''\n",
    "        self.en = en\n",
    "        self.cr = cr\n",
    "        # Task: define your encoder RNN, decoder RNN and the predictor by modifying the code below\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.predictor = None\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor = None):\n",
    "        '''\n",
    "        Compute the class/letter distribution during training and one-hot vectors during testing.\n",
    "        Note that while in general, we stop generating the text only when EOS is generated, we \n",
    "        always generate output sequences of the same lengths as the input sequences for simplicity\n",
    "        in this question.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a batch_size x length x num_char_cr one-hot tensor representation for batch_size input \n",
    "           texts, each of the same length, and where num_char_cr is the number of letters in the\n",
    "           cryptic language\n",
    "        y: None during testing, and a batch_size x length x num_char_en tensor during training, \n",
    "           corresponding to the one-hot representation for the English translation of the cryptic \n",
    "           texts.\n",
    "\n",
    "        Returns:\n",
    "        output: a batch_size x length x num_char_en tensor, which represents the class probabilities \n",
    "           output by the predictor g during training, and the one-hot representation of the English \n",
    "           translation of the texts using the greedy strategy described in class.\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        # Note: reversing the input sequence before feeding it to the encoder is often helpful\n",
    "\n",
    "        # HELP:\n",
    "        # - write down the input shape and output shape for the encoder, decoder and predictor on paper first\n",
    "        # - print the input shape and output shape for the encoder, decoder and predictor in your code and \n",
    "        #   compare with your answers on paper\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def translate(self, x: list[str]):\n",
    "        '''\n",
    "        Return a list of English texts of a list of cryptic texts.\n",
    "        '''\n",
    "        # HELP\n",
    "        # If you use the code below as it is, you need to also allow the forward function to \n",
    "        # accept a list of onehot sequences output by the onehot function. This can be achieved \n",
    "        # by calling x = torch.stack(x) at the beginning of forward.\n",
    "        # Alternatively, you can change the first line below to x = torch.stack(self.cr.onehot(x))\n",
    "        # In general, you can interpret list, array, and tensor in the docstring as synomymous \n",
    "        # and choose whichever you find easier to work with.\n",
    "        x = self.cr.onehot(x) \n",
    "        y = self(x)\n",
    "        y = self.en.text(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e0506-34df-4bf3-841d-632a0a337030",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELP: initialize your Translator and do something like the following to test it\n",
    "# net = Translator(en, cr)\n",
    "# net(x_tr_oh[:10]) # assume x_tr_oh is the onehot tensor verson of x_tr\n",
    "# net(x_tr_oh[:10], y_tr_oh[:10]) # assume y_tr_oh is the onehot tensor version of y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-reasoning",
   "metadata": {},
   "source": [
    "**(c)** (15 marks)\n",
    "Implement a train function and train your model.\n",
    "Describe details in the training procedure so that someone reading your\n",
    "report are able to reproduce how you train the model.\n",
    "That is, you need to include details like the choice of optimizer together\n",
    "with the hyperparameters used, the batch size, the initial hidden state.\n",
    "\n",
    "For your final model, output its translations along with the given translations for the first three test examples.\n",
    "In addition, use the `jaccard` function to score your model's translations on the training and test sets.\n",
    "5 marks are allocated for your model performance: your model receives $5 \\times \\min(1, \\text{test score}/0.85)$ marks.\n",
    "Note that training a good model can take time, and it is better to figure out how to train a good model on a small subset first before trying to train a good model on the full dataset.\n",
    "\n",
    "Save your final model using `torch.save` in the `supplements` and name your model as `mt.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-crest",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666c359-4ed5-4eee-9da9-ef45e1d643d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELP: if you compute your loss using PyTorch's CrossEntropyLoss, make sure you read \n",
    "## the documentation carefully and compute your loss correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465d7e6b-65ad-492d-8cd2-e6be8d9b75c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(y_true: list[str], y_pred: list[str]):\n",
    "    ncorrect = 0.\n",
    "    npred = 0.\n",
    "    ntrue = 0.\n",
    "    for i in range(len(y_true)):\n",
    "        ntrue += len(y_true[i])\n",
    "        npred += len(y_pred[i])\n",
    "        for t in range(min(len(y_true[i]), len(y_pred[i]))):\n",
    "            ncorrect += (y_pred[i][t] == y_true[i][t])\n",
    "    return 2*ncorrect/(npred + ntrue)\n",
    "\n",
    "# example usage\n",
    "y_true = ['this is a correct translation', 'this is another correct translation']\n",
    "y_pred = ['this is a norrect translatiom', 'that is another dorrect sranslation']\n",
    "jaccard(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
