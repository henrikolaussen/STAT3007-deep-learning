{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-transsexual",
   "metadata": {
    "id": "joTz10PJRPwN"
   },
   "source": [
    "<center>\n",
    "    <H1>STAT3007 Deep Learning, Assignment 2</H1>\n",
    "    <b>2024 Semester 1, due 5pm on 29 Apr</b>\n",
    "</center>\n",
    "\n",
    "Please read `instructions.ipynb` first.\n",
    "\n",
    "**Name**: [Your name]\n",
    "<br>\n",
    "**Student Number**: [Your student number]\n",
    "\n",
    "$\\newcommand{\\reals}{{\\mathbf R}}$\n",
    "$\\newcommand{\\bfx}{{\\mathbf x}}$\n",
    "$\\newcommand{\\norm}[1]{\\|#1\\|}$\n",
    "$\\newcommand{\\var}{\\text{Var}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384a7c8-f876-4f24-973e-9d7cffd22538",
   "metadata": {},
   "source": [
    "## Q1. Optimization (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45f8ed-699d-460a-a279-98240da620f4",
   "metadata": {},
   "source": [
    "**(a)** (5 marks) In lecture, we introduced the back-propagation algorithm for MLPs and \n",
    "mentioned that convolutional neural nets can be viewed as special cases for \n",
    "MLPs.\n",
    "Can the back-propagation formula for MLPs be directly applied to CNNs?\n",
    "Justify your answer; if your answer is no, also describe what changes are \n",
    "needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-estonia",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-dance",
   "metadata": {},
   "source": [
    "**(b)** (5 marks) Let $f(x, y) = \\sin(xy) + \\cos(x) \\cos(y)$.\n",
    "    Use reverse mode autodiff to compute the value of the partial derivative $\\frac{\\partial f}{\\partial x}$ at $(x, y) = (1, 1)$.\n",
    "    Show your computational graph, and tabulate the intermediate derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-trade",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74681f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mq/21m9lg3j3vx6wns8srwq8v4m0000gn/T/ipykernel_6639/2167598822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "\n",
    "def f(x,y):\n",
    "    return torch.sin(x*y) + torch.cos(x)*torch.cos(y)\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad= True)\n",
    "y = torch.tensor(1.0, requires_grad= True)\n",
    "\n",
    "out = f(x,y)\n",
    "out.backward()\n",
    "make_dot(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944cf70b-4090-4452-9bf8-e5e06d6f1be9",
   "metadata": {},
   "source": [
    "**(c)**. (5 marks) Plot the univariate function $f: \\mathbf{R} \\to \\mathbf{R}$ defined by  $f(x) = \\tanh(x^3 + 1.5*\\sin(3 \\pi x))$ over the interval $[-5, 5]$.\n",
    "Describe two difficulties for minimizing $f$ using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a03578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return torch.tanh(x**3+1.5*torch.sin(3*torch.pi*x))\n",
    "\n",
    "x = torch.linspace(-5,5,100)\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb2e12-1f24-4b11-a3e8-ea93df158263",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a5774",
   "metadata": {},
   "source": [
    "The first difficulty is that gradient descent can't guarantee a global solution. This means that saddle points and local minimas are possible solution obtained by gradient descent. Secondly, we have the problem of exploding and vanishing gradients. A vanishing gradient means that the computed gradients are very small in value. As a consequence, gradient descent might converge very slowly towards a solution. Moreover, in the case of an exploding gradient, the gradient is very large in value. This could cause gradient descent to overshoot a possible solution by making excessively large steps in one or more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd8459-0e5d-4458-9c04-3fa2e1f6d75e",
   "metadata": {},
   "source": [
    "**(d)** (10 marks) Let $\\alpha > 0$ be a constant, and \n",
    "$(x)_{+,\\alpha} = \\begin{cases} x, & x \\ge 0 \\\\ \\alpha x, & x < 0\\end{cases}$\n",
    "denote the leaky ReLU with its slope for the negative part being $\\alpha$.\n",
    "\n",
    "Consider $Z = \\sum_{i=1}^{n} (Y_{i})_{+,\\alpha} W_{i}$, where $Y_{i}$'s are independent copies of a random variable $Y$ symmetrically distributed around 0, and $W_{i}$'s are independent copies of the random variable $W \\sim N(0, \\sigma^{2})$.\n",
    "\n",
    "Find $\\sigma^{2}$ such that $\\var(Z) = \\var(Y)$. You can use results covered in lectures and the prerequisite courses. All other steps must be justified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf04018-0d41-4620-a44e-97109b8eaa76",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-signal",
   "metadata": {},
   "source": [
    "## Q2. Fashion Networks (45 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-minneapolis",
   "metadata": {},
   "source": [
    "We implement several neural network models for the FashionMNIST dataset in this question. The dataset contains a collection of Zalando's article images. The code below loads the FashionMNIST dataset and shows some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from util import *\n",
    "        \n",
    "# if you don't have FashionMNIST downloaded, run the code below twice to get rid of verbose outputs\n",
    "fashion_tr = torchvision.datasets.FashionMNIST('/Users/henrikolaussen/Desktop/UQ/vår/Deep Learning/Assignments /A2/data', train=True, download=True)\n",
    "fashion_ts = torchvision.datasets.FashionMNIST('/Users/henrikolaussen/Desktop/UQ/vår/Deep Learning/Assignments /A2/data', train=False, download=True)\n",
    "\n",
    "# input values are normalized to [0, 1]\n",
    "x_tr, y_tr = fashion_tr.data.float()/255, fashion_tr.targets\n",
    "x_ts, y_ts = fashion_ts.data.float()/255, fashion_ts.targets\n",
    "\n",
    "print(x_tr.shape, y_tr.shape, x_ts.shape, y_ts.shape)\n",
    "\n",
    "classes = fashion_tr.classes\n",
    "\n",
    "plot_gallery([x_tr[i] for i in range(10)], \n",
    "             titles=[classes[y_tr[i]] for i in range(10)],\n",
    "             xscale=1.5, yscale=1.5, nrow=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-portugal",
   "metadata": {},
   "source": [
    "### Logistic Regression (0 marks)\n",
    "\n",
    "**(a)** As a baseline, the following code trains a logistic regression model on the training set and report its accuracies on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7916057-d79b-400d-85a2-db61985d4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def vec(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(vec(x_tr), y_tr)\n",
    "\n",
    "print('Accuracy: train/test = %.3f / %.3f' % (lr.score(vec(x_tr), y_tr), lr.score(vec(x_ts), y_ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-planning",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-glenn",
   "metadata": {},
   "source": [
    "For training an MLP, we flatten each image to a vector, and convert the labels to their one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "def vec(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "x_tr_mlp, y_tr_mlp = vec(x_tr), one_hot(y_tr)\n",
    "x_ts_mlp, y_ts_mlp = vec(x_ts), one_hot(y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-nudist",
   "metadata": {},
   "source": [
    "**(b)** (5 marks) We considering training a two-hidden-layer MLP with 50 ReLU units for each hidden layer, and 10 output units:\n",
    "\n",
    "\\begin{align*}\n",
    "    f(\\mathbf{x}; W_{1}, W_{2}, W_{3}, b_{1}, b_{2}, b_{3}) \n",
    "    = \n",
    "    \\text{softmax}(W_{3} g(W_{2} g(W_{1} \\mathbf{x} + b_{1}) + b_{2}) + b_{3}),\n",
    "\\end{align*}\n",
    "\n",
    "where each \n",
    "$\\mathbf{x} \\in \\mathbf{R}^{d}$ is the vector representation of a digit image, \n",
    "$W_{1} \\in \\reals^{50 \\times d}$,\n",
    "$W_{2} \\in \\reals^{25 \\times 50}$,\n",
    "$W_{3} \\in \\reals^{10 \\times 25}$ are the weight matrices, \n",
    "$b_{1} \\in \\reals^{50}, b_{2} \\in \\reals^{25}$ and $b_{3} \\in \\reals^{10}$ are the biases, and \n",
    "$g$ is ReLU.\n",
    "Complete the code below according to the docstring to implement this two-hidden-layer MLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-andrews",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self, init='rand'):\n",
    "        '''\n",
    "        Initialize neural network parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        init: either 'zero' (all parameters are 0) or 'rand' (each parameter is uniformly sampled from [-0.5, 0.5])\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        d = 28*28\n",
    "\n",
    "        # Task: add your initialization code below. You may want to use nn.Parameter. For example, \n",
    "        #   self.W = nn.Parameter(torch.randn(100, 10)) \n",
    "        # makes the parameters accessible via self.parameters()\n",
    "        if init == 'zero': # all parameters initialised to 0\n",
    "            self.W1 = nn.Parameter(torch.zeros(50,d))\n",
    "            self.W2 = nn.Parameter(torch.zeros(25,50))\n",
    "            self.W3 = nn.Parameter(torch.zeros(10,25))\n",
    "            self.b1 = nn.Parameter(torch.zeros(50)) \n",
    "            self.b2 = nn.Parameter(torch.zeros(25)) \n",
    "            self.b3 = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "        elif init == 'rand': # all parameters randomly drawn from N(0, 0.1^2) \n",
    "            self.W1 = nn.Parameter(torch.randn(50,d)*0.1)\n",
    "            self.W2 = nn.Parameter(torch.randn(25,50)*0.1)\n",
    "            self.W3 = nn.Parameter(torch.randn(10,25)*0.1)\n",
    "            self.b1 = nn.Parameter(torch.randn(50)*0.1)\n",
    "            self.b2 = nn.Parameter(torch.randn(25)*0.1)\n",
    "            self.b3 = nn.Parameter(torch.randn(10)*0.1)\n",
    "        else:\n",
    "            raise BaseException('Unsupported weight initialization method!')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Compute the outputs for given inputs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a tensor of shape (n_samples, 784)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        o: a tensor of shape (n_samples, 10) containing the class scores\n",
    "        '''\n",
    "        # Task: add your forward computation code below\n",
    "        #o = torch.zeros(len(x),10)\n",
    "        layer1 = nn.functional.relu(torch.matmul(x, self.W1.T)+self.b1)\n",
    "        layer2 = nn.functional.relu(torch.matmul(layer1, self.W2.T)+self.b2)\n",
    "        o = nn.functional.softmax(torch.matmul(layer2, self.W3.T) + self.b3)\n",
    "        return o\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Predict the class indices for the input examples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a tensor of shape (n_samples, 784)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        l: a tensor of shape (n_samples,) consisting of class indices\n",
    "        '''\n",
    "        o = self(x) # the same as self.forward(x)\n",
    "        l = torch.max(o, 1)[1]\n",
    "        return l\n",
    "            \n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Compute the model's accuracy on the given dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a tensor of shape (n_samples, 784)\n",
    "        y: a tensor of shape (n_samples, 10) containing the one-hot encodings\n",
    "        '''\n",
    "        pred_cls = self.predict(x)\n",
    "        true_cls = torch.max(y, 1)[1]\n",
    "        return (pred_cls == true_cls).sum().float().item() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-douglas",
   "metadata": {},
   "source": [
    "**(c)** (15 marks)\n",
    "We study the effect of initialization, learning rates, and batch sizes when training the above MLP by minimizing the following quadratic loss\n",
    "\\begin{align*}\n",
    "  R_{n}({\\bf w})\n",
    "  = \\frac{1}{n} \\sum_{i=1}^{n} ||f(\\mathbf{x}_{i}; {\\bf w}) - y_{i}||_{2}^{2},\n",
    "\\end{align*}\n",
    "where $f({\\bf x}, {\\bf w})$ is the MLP with ${\\bf w}$ as the parameters and ${\\bf x}$ as the input, and\n",
    "$(\\mathbf{x}_{1}, y_{1}), \\ldots (\\mathbf{x}_{n}, y_{n}) \\in \\mathbf{R}^{d} \\times\n",
    "\\mathbf{R}^{10}$ is the training set\n",
    "\n",
    "Train the MLP by running SGD for 100 epochs (one epoch means one pass through the training data) using all combinations of the following hyperparameter values:\n",
    "* initializaton: `zero`, `rand`\n",
    "* learning rate: 0.001, 0.01, 0.1,  1\n",
    "* batch size: 100, 60000\n",
    "\n",
    "Compute the training and test accuracies for all these 16 models, and comment on the effect of initialization, learning rate and batch size.\n",
    "Write your code by completing the partial code provided below. The generic `train` and `test` functions can be used for training CNNs later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-consumer",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseloss(o, y):\n",
    "    '''\n",
    "    Compute the MSE loss defined in the question.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    o: a tensor of shape (n_samples, 10) containing the class scores\n",
    "    y: a tensor of shape (n_samples, 10) containing the one-hot encodings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    l: the MSE loss\n",
    "    '''\n",
    "    # Task:  compute the loss - make sure your code computes exactly the required loss\n",
    "    l = torch.mean((o-y)**2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "def train(net, x, y, lossfunc, lr=0.1, momentum=0, batch_size=600, nepochs=10):\n",
    "    device = next(net.parameters()).device # check what device the net parameters are on\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # training loop\n",
    "    dataloader = DataLoader(DatasetWrapper(x, y), batch_size=batch_size, shuffle=True)\n",
    "    loop = tqdm(range(nepochs), ncols=110)\n",
    "    for i in loop: # for each epoch\n",
    "        t0 = time()\n",
    "        \n",
    "        # Task: fill in your training code below and compute epoch_loss (the average loss on all batches in a epoch)\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0 \n",
    "        for (x_batch, y_batch) in dataloader: # for each mini-batch\n",
    "            optimizer.zero_grad()\n",
    "            o_batch = net.forward(x_batch)\n",
    "            \n",
    "            l = lossfunc(o_batch, y_batch)\n",
    "            epoch_loss += l.item()\n",
    "\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            n_batches += 1\n",
    "\n",
    "        epoch_loss /= n_batches\n",
    "\n",
    "        # evaluate network performance\n",
    "        acc = test(net, x, y, batch_size=batch_size)\n",
    "\n",
    "        # show training progress\n",
    "        loop.set_postfix(loss=\"%5.5f\" % (epoch_loss),\n",
    "                         train_acc=\"%.2f%%\" % (100*acc))\n",
    "\n",
    "# try running test(FashionMLP(), x_ts_mlp, y_ts_mlp, showerrors=True) to see what the code does\n",
    "def test(net, x, y, batch_size=600, showerrors=False):\n",
    "    with torch.no_grad(): # disable automatic gradient computation for efficiency\n",
    "        device = next(net.parameters()).device\n",
    "\n",
    "        pred_cls = []\n",
    "        # make predictions on mini-batches  \n",
    "        dataloader = DataLoader(DatasetWrapper(x), batch_size=batch_size, shuffle=False)\n",
    "        for x_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            pred_cls.append(torch.max(net(x_batch), 1)[1].cpu())\n",
    "\n",
    "        # compute accuracy\n",
    "        pred_cls = torch.cat(pred_cls) # concat predictions on the mini-batches\n",
    "        true_cls = torch.max(y, 1)[1].cpu()\n",
    "        acc = (pred_cls == true_cls).sum().float() / len(y)\n",
    "\n",
    "        # show errors if required\n",
    "        if showerrors:\n",
    "            idx_errors = (pred_cls != true_cls)\n",
    "\n",
    "            x_errors = x[idx_errors][:10].cpu()\n",
    "            y_pred = pred_cls[idx_errors][:10].cpu().numpy()\n",
    "            y_true = true_cls[idx_errors][:10].cpu().numpy()\n",
    "\n",
    "            plot_gallery(x_errors.squeeze(),\n",
    "                         titles=[classes[y_true[i]] + '\\n->' + classes[y_pred[i]] for i in range(10)],\n",
    "                         xscale=1.5, yscale=1.5, nrow=2)\n",
    "\n",
    "        return acc        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "import pandas as pd\n",
    "\n",
    "# Task: write your code to train the 16 models and compute their training and test accuracies\n",
    "\n",
    "initializations = ['zero','rand']\n",
    "learning_rates = [0.001, 0.01, 0.1,  1]\n",
    "batch_sizes = [100, 60000]\n",
    "\n",
    "for init in initializations:\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            net = FashionMLP(init=init)\n",
    "            train(net, x_tr_mlp, y_tr_mlp, lossfunc = mseloss, lr=lr, batch_size=bs, nepochs=100)\n",
    "            print(f'Initialisation: {init}, learning rate: {lr}, batch size: {bs}')\n",
    "            print(f'Train accuracy: {test(net, x_tr_mlp, y_tr_mlp)}')\n",
    "            print(f'Test accuracy: {test(net, x_ts_mlp, y_ts_mlp)}')\n",
    "            print('------------')\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-entertainment",
   "metadata": {},
   "source": [
    "### Convolution neural networks (25 marks)\n",
    "\n",
    "For training a CNN, we use one-hot encodings of the labels as for the MLP, but we add a channel dimension to the data using the `unsqueeze` function, which will be convenient for performing convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_cnn, y_tr_cnn = x_tr.unsqueeze(1), one_hot(y_tr)\n",
    "x_ts_cnn, y_ts_cnn = x_ts.unsqueeze(1), one_hot(y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-lewis",
   "metadata": {},
   "source": [
    "**(d)** (5 marks) Consider the following CNN:\n",
    "The input layer is followed by the following layers:\n",
    "a convolutional layer with 64 3x3 filters (stride 1) and sigmoid activation,\n",
    "a 2x2 average pooling layer,\n",
    "a fully connected layer with 50 neurons and sigmoid activation,\n",
    "and finally, a fully connected output layer with 10 neurons and identity activation.\n",
    "Complete the code below to implement your CNN in PyTorch.\n",
    "Pytorch's nn.Sequential and nn.Conv2d are helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-tours",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "    \n",
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        # Task: write your class initialization code below\n",
    "        # implement your CNN such that it is easy to extract features for answering (f)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Task: write your forward computation code below\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-number",
   "metadata": {},
   "source": [
    "**(e)** (5 marks)\n",
    "Train a FashionCNN. Use an appropriate loss function and appropriate hyperparameters (e.g., the learning rate, the number of epochs, the batch size).\n",
    "Briefly describe how you make these decisions.\n",
    "Report the test set accuracy of your final CNN and display 10 of its errors on the test set using the `test` function provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-cleaner",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-metallic",
   "metadata": {},
   "source": [
    "**(f)** (10 marks)\n",
    "For the second test image, retrieve its nearest 5 training examples in the\n",
    "learned feature space (the learned features are the inputs to the output\n",
    "layer, that is, the output values of the last hidden layer) for the final model in (e).\n",
    "Do they represent the same kind of clothes as the test image? \n",
    "You can use any suitable distance measure to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-citation",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698924d-3846-4af5-aebd-1ca310382521",
   "metadata": {},
   "source": [
    "**(g)** (5 marks) Use PCA to project the learned test set feature vectors in (e) to a 2D space.\n",
    "Generate the scatter plot of these 2D projections and color points for different classes using different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af2480-30af-4e85-809d-4eab23c04ddc",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bbbcf-eef2-4177-9d25-5f8ba9a2dd46",
   "metadata": {},
   "source": [
    "## Q3. Translating a cryptic language (30 marks)\n",
    "In a fictitious world, a group of scientists obtained a document containing a sample of texts written in a cryptic language, together with their English translations. However, they are unable to find a way of translating arbitrary texts from the cryptic language. Knowing that you have learned deep learning models for machine translation, they are approaching you for help.\n",
    "\n",
    "They have sent you a data file containing a training set and a test set.\n",
    "The code below illustrates how to use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a973d9-e90f-42e9-8443-87404589452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "data = pkl.load(open('cryptic.pkl', 'rb'))\n",
    "x_tr = data['train']['x']\n",
    "y_tr = data['train']['y']\n",
    "x_ts = data['test']['x']\n",
    "y_ts = data['test']['y']\n",
    "print('%d training pairs and %d test pairs\\n' % (len(x_tr), len(x_ts)))\n",
    "\n",
    "print('The third pair of training example:')\n",
    "print('- Cryptic:', x_tr[2])\n",
    "print('- English:', y_tr[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034e590-034c-4648-ad46-e50c4f1899aa",
   "metadata": {},
   "source": [
    "As you can see, these texts are not necessarily complete sentences, and in fact, some words may be incomplete too.\n",
    "\n",
    "In this question, you will train a character-level machine translator using the encoder-decoder architecture as described in Lecture 18 - that is, the encoder takes in one letter at a time from the text in the cryptic language, and the decoder generates one letter at a time for its English translation.\n",
    "For this dataset, each short text, whether in the cryptic language or in English, has exactly 50 letters, which is convenient for batching processing using recurrent models.\n",
    "\n",
    "Specifically, as described in lecture, the encoder-decoder architecture has an encoder RNN $f^{e}$, a decoder RNN $f^{d}$, and a predictor $g$: \n",
    "\\begin{align*}\n",
    "    h^{e}_{t} &= f^{e}(h_{t-1}^{e}, x_{t}), \\\\\n",
    "    h^{d}_{t} &= f^{d}(h_{t}^{d}, y_{t-1}, c), \\\\\n",
    "    y_{t} &\\sim g(\\cdot | h_{t}^{d}, y_{t-1}, c).\n",
    "\\end{align*}\n",
    "For this problem, each input $x_{t}$ is the one-hot vector for a single letter in the cryptic text,\n",
    "each output $y_{t}$ is the one-hot vector for a single letter in the English text.\n",
    "As in general, the context vector $c$ is the last hidden state from the encoder, \n",
    "and the input to the decoder at the $t$-th time step is $(y_{t-1}, c)$.\n",
    "The decoder RNN behaves different during training and testing:\n",
    "* During training, each $y_{t-1}$ in the input $(y_{t-1}, c)$ to the decoder is the one-hot vector representing the $(t-1)$-th letter in the given English translation (assume $t=1$ is the first letter), except that $y_{0}$ is a one-hot vector representing a special SOS (start of sentence) letter.\n",
    "* During testing, each $y_{t-1}$ in the input $(y_{t-1}, c)$ to the decoder is obtained in a greedy way as the one-hot vector for the most likely letter predicted by the predictor $g$, except that $y_{0}$ represents SOS.\n",
    "\n",
    "**(a)** (5 marks)\n",
    "Complet the code for the `Lang` class below.\n",
    "Append an EOS letter '\\n' to each example cryptic/English text.\n",
    "Create `Lang` objects for both the cryptic language and English, and use them to convert texts to one-hot representations for both the training and test data.\n",
    "Make sure you add an additional SOS letter defined by `chr(0)` as an additional letter for the English language, as this is needed for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-utilization",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54e7d5-835d-4736-8b3b-e3b5cffc5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, text):\n",
    "        self.char2int = dict(enumerate(sorted(set(text))))\n",
    "        self.int2char = {v: k for k, v in self.char2int.items()}\n",
    "        self.num_char = len(self.char2int)\n",
    "\n",
    "    def onehot(self, texts):\n",
    "        '''\n",
    "        Convert a list of strings to a list of sequences of one-hot vectors for letters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        texts: a list or array of str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a list, where output[i] is a list of one-hot vectors for the letters in texts[i]\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        pass\n",
    "\n",
    "    def text(self, onehots):\n",
    "        '''\n",
    "        Convert a list of sequences of one-hot vectors to a list of strings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        onehots: a list, where onehots[i] is a list of one-hot vectors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: a list, where output[i] is a str corresponding to sequence of one-hot vectors in onehots[i]\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        pass\n",
    "\n",
    "# Task: create the Lang objects for the cryptic language and English, and convert the data to one-hot representations\n",
    "SOS = chr(0)\n",
    "EOS = '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-things",
   "metadata": {},
   "source": [
    "**(b)** (10 marks)\n",
    "Complete the code for the encoder-decoder model below.\n",
    "Write a short paragraph to briefly describe the type of architecture used\n",
    "(e.g. vanilla RNN, LSTM, GRU, MLP) for the encoder/decoder/predictor, and the hyperparameters chosen \n",
    "(e.g. number of layers, number of neurons for each layer, activation functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-binary",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import LSTM\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    '''\n",
    "    Your code should support usages as illustrated in the code below, and you should run such \n",
    "    code to see if your model has any bug before proceeding to train it.\n",
    "    \n",
    "    net = Translator(en, cr)\n",
    "    # input the one-hot representation of some cryptic texts and their translations, and output\n",
    "    # the class probabilities at each time step.\n",
    "    p = net(x, y)\n",
    "    # input the one-hot representation of some cryptic texts, and output one-hot representation\n",
    "    # of their translations\n",
    "    yhat = net(x)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, en: Lang, cr: Lang):\n",
    "        '''\n",
    "        You can add additional arguments to this function if you want.\n",
    "        '''\n",
    "        self.en = en\n",
    "        self.cr = cr\n",
    "        # Task: define your encoder RNN, decoder RNN and the predictor by modifying the code below\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.predictor = None\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor = None):\n",
    "        '''\n",
    "        Compute the class/letter distribution during training and one-hot vectors during testing.\n",
    "        Note that while in general, we stop generating the text only when EOS is generated, we \n",
    "        always generate output sequences of the same lengths as the input sequences for simplicity\n",
    "        in this question.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a batch_size x length x num_char_cr one-hot tensor representation for batch_size input \n",
    "           texts, each of the same length, and where num_char_cr is the number of letters in the\n",
    "           cryptic language\n",
    "        y: None during testing, and a batch_size x length x num_char_en tensor during training, \n",
    "           corresponding to the one-hot representation for the English translation of the cryptic \n",
    "           texts.\n",
    "\n",
    "        Returns:\n",
    "        output: a batch_size x length x num_char_en tensor, which represents the class probabilities \n",
    "           output by the predictor g during training, and the one-hot representation of the English \n",
    "           translation of the texts using the greedy strategy described in class.\n",
    "        '''\n",
    "        # Task: implement this function according to the docstring\n",
    "        # Note: reversing the input sequence before feeding it to the encoder is often helpful\n",
    "        pass\n",
    "\n",
    "    def translate(self, x: list[str]):\n",
    "        '''\n",
    "        Return a list of English texts of a list of cryptic texts.\n",
    "        '''\n",
    "        x = self.cr.onehot(x)\n",
    "        y = self(x)\n",
    "        y = self.en.text(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-reasoning",
   "metadata": {},
   "source": [
    "**(c)** (15 marks)\n",
    "Implement a train function and train your model.\n",
    "Describe details in the training procedure so that someone reading your\n",
    "report are able to reproduce how you train the model.\n",
    "That is, you need to include details like the choice of optimizer together\n",
    "with the hyperparameters used, the batch size, the initial hidden state.\n",
    "\n",
    "For your final model, output its translations along with the given translations for the first three test examples.\n",
    "In addition, use the `jaccard` function to score your model's translations on the training and test sets.\n",
    "5 marks are allocated for your model performance: your model receives $5 \\times \\min(1, \\text{test score}/0.85)$ marks.\n",
    "Note that training a good model can take time, and it is better to figure out how to train a good model on a small subset first before trying to train a good model on the full dataset.\n",
    "\n",
    "Save your final model using `torch.save` in the `supplements` and name your model as `mt.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-crest",
   "metadata": {},
   "source": [
    "**Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d7e6b-65ad-492d-8cd2-e6be8d9b75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(y_true: list[str], y_pred: list[str]):\n",
    "    ncorrect = 0.\n",
    "    npred = 0.\n",
    "    ntrue = 0.\n",
    "    for i in range(len(y_true)):\n",
    "        ntrue += len(y_true[i])\n",
    "        npred += len(y_pred[i])\n",
    "        for t in range(min(len(y_true[i]), len(y_pred[i]))):\n",
    "            ncorrect += (y_pred[i][t] == y_true[i][t])\n",
    "    return 2*ncorrect/(npred + ntrue)\n",
    "\n",
    "# example usage\n",
    "y_true = ['this is a correct translation', 'this is another correct translation']\n",
    "y_pred = ['this is a norrect translatiom', 'that is another dorrect sranslation']\n",
    "jaccard(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
